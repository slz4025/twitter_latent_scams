{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet2tokens",
      "provenance": [],
      "collapsed_sections": [
        "hm8vUSmlK-tr",
        "7w9nf8yxGaFp",
        "Q_IJIm-mK3ZO",
        "zp4IUlHIEEd-",
        "SHqCDbLgKqg9",
        "TG_4EDYzHENA",
        "eGB63UkOKZNz",
        "ZEh4E02EKdwg",
        "i75pWxBb_P1L",
        "Gk7WFp6mAgVi",
        "FuQVCgCY-2Rd"
      ],
      "authorship_tag": "ABX9TyNoucHcofEWhXzJU5su7Ybu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slz4025/twitter_latent_scams/blob/master/tweet2tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b72n8C-FwGtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p4kzZnpxTFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"with_score_filtered_covid_20200602\" #@param {type: \"string\"}\n",
        "analyzed_dir = \"drive/My Drive/latent_scams/analyzed/\"#@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPU3TOhgLnTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKRseazaAauQ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpa3gLZLHl1A",
        "colab_type": "text"
      },
      "source": [
        "Open and select the desired unit for processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Y438oYHSDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_pd = pd.read_csv(analyzed_dir + file_name + \".tsv\", sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5AEMHuNJy8H",
        "colab_type": "text"
      },
      "source": [
        "###Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTkSW_A6MQ87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_docs = scored_tweets_pd[\"body\"]\n",
        "clean_pd = scored_tweets_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLB5uj-L8cV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsVH8dC3_8Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taking out !?#$@'\n",
        "modified_punc = \"\"\"\n",
        ".,\"&()*+-/:;<=>[\\]^_`{|}~\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Oigmst70Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
        "def de_emojify(text):\n",
        "    regex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regex_pattern.sub(r'', text)\n",
        "\n",
        "#https://help.twitter.com/en/using-twitter/replies-not-showing-up-and-hashtag-problems\n",
        "# (1) words with opt. apostrophe\n",
        "# (2) words with opt. @ prefix\n",
        "# (3) words with opt. # prefix\n",
        "pattern = r\"\"\"\\w+(?:'\\w+)*|@?\\w+|#?\\w+\"\"\"\n",
        "\n",
        "#https://www.nltk.org/book/ch03.html#sec-tokenization\n",
        "def tweet_tokenize(text):\n",
        "  return nltk.regexp_tokenize(text, pattern)\n",
        "\n",
        "def clean_body(body):\n",
        "  body = body.lower()\n",
        "  de_punc = body.translate(str.maketrans('', '', modified_punc))\n",
        "  de_emoji = de_emojify(de_punc)\n",
        "  tokens = tweet_tokenize(de_emoji) \n",
        "  de_stop = [word for word in tokens if word not in stopwords.words('english')]\n",
        "  rem_url = [word for word in tokens if word[:4] != \"http\"]\n",
        "  num_tok = [\"NUM\" if word.isdigit() else word for word in tokens]\n",
        "  return num_tok\n",
        "\n",
        "# test\n",
        "print(clean_body(\"We'll see about that @missy #revenge\"))\n",
        "print(clean_body(\".,!$'?\\\"&()*+-/:;<=>[]^_`{|}~#@'\"))\n",
        "print(clean_body(\"U.S.A U.S.A. u.s.a u.s.a. U.S. U.\"))\n",
        "print(clean_body(\"He @bud It's a long walk homeðŸ˜¢ #tears.\"))\n",
        "print(clean_body(\"https://tco.com/3948hJKL*efoW\"))\n",
        "print(clean_body(\"I've got $12.00 and 40% people want it.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0STuFtmW7EtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned = []\n",
        "for c in clean_docs:\n",
        "  cleaned.append(clean_body(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-byxWXz_xO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_prefix = \"processing_all\"#@param {type: \"string\"}\n",
        "cleaned_file_name = selected_prefix + \"_\" + file_name + \".tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omSWCJ3haQer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_cleaned = [' '.join(c) for c in cleaned]\n",
        "clean_pd[\"cleaned\"] = _cleaned\n",
        "clean_pd.to_csv(sep='\\t', path_or_buf=analyzed_dir + cleaned_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kWYj2b4V54g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_pd = pd.read_csv(analyzed_dir + cleaned_file_name, sep=\"\\t\")\n",
        "_cleaned = clean_pd[\"cleaned\"]\n",
        "cleaned = [c.split(' ') for c in _cleaned]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGxIEA_iLmIx",
        "colab_type": "text"
      },
      "source": [
        "###Phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PRNOyUs5DbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrases_docs = cleaned\n",
        "phrases_pd = clean_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsUQIkIw9bjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# phrases\n",
        "from gensim.models import (\n",
        "    phrases,\n",
        "    Phrases,\n",
        ")\n",
        "bigram = Phrases(phrases_docs, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = Phrases(bigram[phrases_docs], threshold=100) \n",
        "\n",
        "bigram_mod = phrases.Phraser(bigram)\n",
        "trigram_mod = phrases.Phraser(trigram)\n",
        "\n",
        "phrases = [trigram_mod[bigram_mod[doc]] for doc in phrases_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJlLUPrMLqnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_prefix = \"processing_all\"#@param {type: \"string\"}\n",
        "phrases_file_name = selected_prefix + \"_\" + file_name + \".tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODKHY_BjaG-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_phrases = [' '.join(p) for p in phrases]\n",
        "phrases_pd[\"phrases\"] = _phrases\n",
        "phrases_pd.to_csv(sep='\\t', path_or_buf=analyzed_dir + phrases_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feMzvvgwVpZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrases_pd = pd.read_csv(analyzed_dir + phrases_file_name, sep=\"\\t\")\n",
        "_phrases = phrases_pd[\"phrases\"]\n",
        "phrases = [p.split(' ') for p in _phrases]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBf606suMKIF",
        "colab_type": "text"
      },
      "source": [
        "###Lemmatization - probably do not need\n",
        "Different stems, especially regarding scams, actually to seem to have significant impact on whether the tweet is relevant or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8b1itsxMpbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrases_pd = pd.read_csv(analyzed_dir + phrases_file_name, sep=\"\\t\")\n",
        "_phrases = clean_pd[\"phrases\"]\n",
        "phrases = [p.split(' ') for p in _phrases]\n",
        "lemma_docs = phrases\n",
        "lemma_pd = phrases_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyMqYKxr791R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "!python3 -m spacy download en\n",
        "import spacy\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for t in texts:\n",
        "        doc = nlp(\" \".join(t)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "lemma = lemmatization(lemma_docs, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T2EIkrAM1fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_prefix = \"processing_trained\"#@param {type: \"string\"}\n",
        "lemma_file_name = selected_prefix + \"_\" + file_name + \".tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WQZWS6EaLot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_lemma = [' '.join(l) for l in lemma]\n",
        "lemma_pd[\"lemma\"] = _lemma\n",
        "lemma_pd.to_csv(sep='\\t', path_or_buf=analyzed_dir + lemma_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzCuh584Vs7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemma_pd = pd.read_csv(analyzed_dir + lemma_file_name, sep=\"\\t\")\n",
        "_lemma = lemma_pd[\"lemma\"]\n",
        "lemma = [l.split(' ') for l in _lemma]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8xm5kEgAd2d",
        "colab_type": "text"
      },
      "source": [
        "## Corpus Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9-489iNGNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_docs = phrases\n",
        "corpus_pd = phrases_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQgwwb9dGJQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
        "from gensim import corpora\n",
        "\n",
        "dictionary = corpora.Dictionary(corpus_docs)\n",
        "rev_dict = {v: k for k, v in dictionary.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu3L6P5TvMEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad_tokens = [\n",
        "  \"covid19\", \"#covid19\", \"coronavirus\", \"#coronavirus\", \"covid\"]\n",
        "bad_ids = [rev_dict[b] for b in bad_tokens if b in rev_dict]\n",
        "dictionary.filter_tokens(bad_ids=bad_ids)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.05, keep_n=100000, keep_tokens=None)\n",
        "dictionary.compactify()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD26u4MQXXnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# double check if filters are reasonable\n",
        "dict_dict = {k: v for k, v in dictionary.items()}\n",
        "freq_list = [(v, dict_dict[k]) for k, v in dictionary.dfs.items()]\n",
        "freq_list.sort(reverse=True)\n",
        "freq_list[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im1AFgy-xGbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = len(dictionary)\n",
        "corpus = list(dictionary.values())\n",
        "N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJXPlFoHuMdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_filtered = [[e for e in c if e in corpus] for c in corpus_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UShJ89GIXg4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_prefix = \"processing_all\"#@param {type: \"string\"}\n",
        "corpus_file_name = selected_prefix + \"_\" + file_name + \".tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOhh2mqrXpEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_corpus_filtered = [' '.join(c) for c in corpus_filtered]\n",
        "corpus_pd[\"corpus_filtered\"] = _corpus_filtered\n",
        "corpus_pd.to_csv(sep='\\t', path_or_buf=analyzed_dir + corpus_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}